{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from yahpo_train.model  import *\r\n",
                "from yahpo_train.metrics import *\r\n",
                "from yahpo_train.cont_scalers import *\r\n",
                "from yahpo_gym.benchmarks import lcbench, rbv2, nasbench_301, fcnet, taskset\r\n",
                "from yahpo_gym.configuration import cfg\r\n",
                "from fastai.callback.wandb import *\r\n",
                "from functools import partial"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Training a surrogate:\r\n",
                "\r\n",
                "We provide a function `fit_config` that allows for training a surrogate with a set of hyperparameters and the option to export the surrogate (this can overwrite existing surrogates!).\r\n",
                "\r\n",
                "A particularity is that we use a set of so called `ContTransformers` in order to transfer continuous variables to a scale better suited for optimization! \r\n",
                "This has a strong effect on the resulting performance and should therefore be optimized!"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "def fit_config(key, embds_dbl=None, embds_tgt=None, tfms=None, lr = 1e-4, epochs=25, deep=[512,512,256], deeper=[], dropout=0., wide=True, use_bn=False, frac=1.0, bs=2048, export=False):\r\n",
                "    \"\"\"\r\n",
                "    Fit function with hyperparameters\r\n",
                "    \"\"\"\r\n",
                "    cc = cfg(key)\r\n",
                "    dls = dl_from_config(cc, bs=bs, frac=frac)\r\n",
                "\r\n",
                "    # Construct embds from transforms. tfms overwrites emdbs_dbl, embds_tgt\r\n",
                "    if tfms is not None:\r\n",
                "        embds_dbl = [tfms.get(name) if tfms.get(name) is not None else ContTransformerNone for name, cont in dls.all_cols[dls.cont_names].iteritems()]\r\n",
                "        embds_tgt = [tfms.get(name) if tfms.get(name) is not None else ContTransformerNone for name, cont in dls.ys.iteritems()]\r\n",
                "\r\n",
                "    # Instantiate learner\r\n",
                "    f = FFSurrogateModel(dls, layers=deep, deeper=deeper, ps=dropout, use_bn = use_bn, wide=wide, embds_dbl=embds_dbl, embds_tgt=embds_tgt)\r\n",
                "    l = SurrogateTabularLearner(dls, f, loss_func=nn.MSELoss(reduction='mean'), metrics=nn.MSELoss)\r\n",
                "    l.metrics = [AvgTfedMetric(mae),  AvgTfedMetric(r2), AvgTfedMetric(spearman)]\r\n",
                "    l.add_cb(MixHandler)\r\n",
                "    l.add_cb(EarlyStoppingCallback(patience=3))\r\n",
                "\r\n",
                "    # Fit\r\n",
                "    l.fit_flat_cos(epochs, lr)\r\n",
                "\r\n",
                "    if export:\r\n",
                "        l.export_onnx(cc)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Example\r\n",
                "\r\n",
                "Find an example for training the `NASBENCH 301` surrogate below:\r\n",
                "\r\n",
                "We supply a list of `ContTransformer`'s to our `fit_config` function that define the specific transformers that should be applied for this scenario:\r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\r\n",
                "def fit_nb301(key = 'nb301', **kwargs):\r\n",
                "    embds_dbl = [partial(ContTransformerMultScalar, m = 1/52)]\r\n",
                "    embds_tgt = [partial(ContTransformerMultScalar, m = 1/100), ContTransformerRange]\r\n",
                "    fit_config(key, embds_dbl=embds_dbl, embds_tgt=embds_tgt, **kwargs)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Example\r\n",
                "\r\n",
                "A more involved example is the `rbv2_super` surrogate, where multiple different transformers are used depending on the input and output variables.\r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\r\n",
                "def fit_rbv2_super(key = 'rbv2_super', **kwargs):\r\n",
                "    # Transforms\r\n",
                "    tfms = {}\r\n",
                "    [tfms.update({k:ContTransformerRange}) for k in [\"mmce\", \"f1\", \"auc\", \"aknn.k\", \"aknn.M\", \"rpart.maxdepth\", \"rpart.minsplit\", \"rpart.minbucket\", \"xgboost.max_depth\"]]\r\n",
                "    [tfms.update({k:partial(ContTransformerLogRange)}) for k in [\"timetrain\", \"timepredict\", \"svm.cost\", \"svm.gamma\"]]\r\n",
                "    [tfms.update({k:partial(ContTransformerLogRange, logfun=torch.log2,  expfun=torch.exp2 )}) for k in [\"glmnet.s\", \"rpart.cp\", \"aknn.ef\", \"aknn.ef_construction\", \"xgboost.nrounds\", \"xgboost.eta\", \"xgboost.gamma\", \"xgboost.lambda\", \"xgboost.alpha\", \"xgboost.min_child_weight\", \"ranger.num.trees\", \"ranger.min.node.size\", 'ranger.num.random.splits']]\r\n",
                "    [tfms.update({k:ContTransformerNegExpRange}) for k in [\"logloss\"]]\r\n",
                "\r\n",
                "    fit_config(key, tfms=tfms, **kwargs)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit (conda)"
        },
        "interpreter": {
            "hash": "935079f3ab4b06ec76910fd5af9cfadee87e8a756fe17d7789065f69c1782d29"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}